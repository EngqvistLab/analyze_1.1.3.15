{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for downloading up-to-date protein domain information for the 1.1.3.15 sequences.<br/><br/>Copyright (C) 2019  Martin Engqvist Lab<br/>This program is free software: you can redistribute it and/or modify<br/>it under the terms of the GNU General Public License as published by<br/>the Free Software Foundation, either version 3 of the License, or<br/>(at your option) any later version.<br/>This program is distributed in the hope that it will be useful,<br/>but WITHOUT ANY WARRANTY; without even the implied warranty of<br/>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br/>GNU General Public License for more details.<br/>You should have received a copy of the GNU General Public License<br/>along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard variables loaded, you are good to go!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from os.path import join, dirname, basename, exists, isdir\n",
    "\n",
    "### Load environmental variables from the project root directory ###\n",
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# now you can get the variables using their names\n",
    "\n",
    "# Check whether a network drive has been specified\n",
    "DATABASE = os.environ.get(\"NETWORK_URL\")\n",
    "if DATABASE == 'None':\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "    #mount network drive here\n",
    "\n",
    "# set up directory paths\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJ = dirname(dotenv_path) # project root directory\n",
    "\n",
    "DATA = join(PROJ, 'data') #data directory\n",
    "RAW_EXTERNAL = join(DATA, 'raw_external') # external data raw directory\n",
    "RAW_INTERNAL = join(DATA, 'raw_internal') # internal data raw directory\n",
    "INTERMEDIATE = join(DATA, 'intermediate') # intermediate data directory\n",
    "FINAL = join(DATA, 'final') # final data directory\n",
    "\n",
    "RESULTS = join(PROJ, 'results') # output directory\n",
    "FIGURES = join(RESULTS, 'figures') # figure output directory\n",
    "PICTURES = join(RESULTS, 'pictures') # picture output directory\n",
    "\n",
    "\n",
    "\n",
    "print('Standard variables loaded, you are good to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First download all UniProt and UniParc pages relating to the identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>ec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>A0A0U5F9V4</td>\n",
       "      <td>1.1.3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>S6DC87</td>\n",
       "      <td>1.1.3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>D4XA33</td>\n",
       "      <td>1.1.3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>D4XA32</td>\n",
       "      <td>1.1.3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>D4XIR1</td>\n",
       "      <td>1.1.3.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           uid        ec\n",
       "34  A0A0U5F9V4  1.1.3.15\n",
       "35      S6DC87  1.1.3.15\n",
       "36      D4XA33  1.1.3.15\n",
       "37      D4XA32  1.1.3.15\n",
       "38      D4XIR1  1.1.3.15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def dlfile(folder, filename, url):\n",
    "    '''\n",
    "    Download a web page if the corresponding file does not exist.\n",
    "    '''\n",
    "\n",
    "    # Open the url\n",
    "    try:\n",
    "        out_path = join(folder, filename)\n",
    "        if not exists(out_path):\n",
    "            f = urlopen(url)\n",
    "            print(\"downloading \" + url)\n",
    "\n",
    "            # Open local file for writing\n",
    "            with open(out_path, \"wb\") as local_file:\n",
    "                local_file.write(f.read())\n",
    "            time.sleep(1)\n",
    "\n",
    "    #handle errors\n",
    "    except HTTPError as e:\n",
    "        print(\"HTTP Error:\", e.code, url)\n",
    "    \n",
    "    except URLError as e:\n",
    "        print(\"URL Error:\", e.reason, url)\n",
    "\n",
    "\n",
    "\n",
    "# load a list of the identifiers I want\n",
    "filepath = join(FINAL, 'brenda_2017_1', 'ec_uid_org_from_fasta_2017_1.tsv')\n",
    "uid_ec = pd.read_csv(filepath, sep='\\t').drop_duplicates()\n",
    "\n",
    "# # only keep 1.1.3.15\n",
    "data_subset = uid_ec[uid_ec['ec']=='1.1.3.15']\n",
    "\n",
    "display(data_subset.head())\n",
    "\n",
    "uids = data_subset.uid.values\n",
    "display(len(uids))\n",
    "\n",
    "# download uniparc and uniprot for each\n",
    "filepath = join(RAW_EXTERNAL, 'pfam')\n",
    "\n",
    "if not exists(filepath):\n",
    "    os.mkdir(filepath)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now look for pfam identifiers in these pages using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_pfam_from_file(filepath):\n",
    "    '''\n",
    "    Parse out pfam data from a file\n",
    "    '''\n",
    "    # open the uniprot and uniparc pages and append them\n",
    "    with open(filepath, 'r') as f:\n",
    "        document = f.read()\n",
    "\n",
    "        \n",
    "    # search through the combined document for identifiers\n",
    "    m = re.findall('(PF[0-9]{5}|CL[0-9]{4})', document)\n",
    "\n",
    "    # loop through the search result and keep unique identifiers\n",
    "    pfam_ids = set([])\n",
    "    for pid in m:\n",
    "        if pid == '':\n",
    "            continue\n",
    "                \n",
    "        pfam_ids.add(pid)\n",
    "        \n",
    "    return pfam_ids\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "data = {'uid':[], 'pfam':[]}\n",
    "    \n",
    "    \n",
    "\n",
    "for uid in uids:\n",
    "    \n",
    "    # download uniprot file\n",
    "    uniprot_url = 'https://www.uniprot.org/uniprot/'\n",
    "    dlfile(folder=filepath, filename='%s_uniprot.html' % uid, url=uniprot_url+uid)\n",
    "    \n",
    "    # query uniprot file\n",
    "    pfam_ids = get_pfam_from_file(join(filepath, '%s_uniprot.html' % uid))\n",
    "\n",
    "    \n",
    "    # download uniparc overview file\n",
    "    uniparc_url = 'https://www.uniprot.org/uniparc/?query=%s' % uid\n",
    "    overview_filename = join(filepath, '%s_uniparc_overview.html' % uid)\n",
    "    dlfile(folder=filepath, filename='%s_uniparc_overview.html' % uid, url=uniparc_url)\n",
    "    \n",
    "    # find the reference for the alternate identifier and download that file\n",
    "    with open(overview_filename, 'r') as f:\n",
    "        document = f.read()\n",
    "        \n",
    "        m = re.search('class=\"entryID\"><a href=\"/uniparc/([a-zA-Z0-9]+)\">', document)\n",
    "        new_target_url = 'https://www.uniprot.org/uniparc/%s' % m.group(1)\n",
    "        dlfile(folder=filepath, filename='%s_uniparc.html' % uid, url=new_target_url)\n",
    "    \n",
    "    # query uniparc file\n",
    "    pfam_ids2 = get_pfam_from_file(join(filepath, '%s_uniparc.html' % uid))\n",
    "\n",
    "            \n",
    "    data['uid'].append(uid)\n",
    "    data['pfam'].append(','.join(sorted(list(pfam_ids.union(pfam_ids2)))))\n",
    "\n",
    "data_frame = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(join(FINAL, 'brenda_2017_1', 'pfam_info_2017_1.tsv'), sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

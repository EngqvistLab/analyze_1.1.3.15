{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for obtaining Pfam domains for all BRENDA sequences.<br/><br/>Copyright (C) 2020-2021  Martin Engqvist Lab<br/>This program is free software: you can redistribute it and/or modify<br/>it under the terms of the GNU General Public License as published by<br/>the Free Software Foundation, either version 3 of the License, or<br/>(at your option) any later version.<br/>This program is distributed in the hope that it will be useful,<br/>but WITHOUT ANY WARRANTY; without even the implied warranty of<br/>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br/>GNU General Public License for more details.<br/>You should have received a copy of the GNU General Public License<br/>along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard variables loaded, you are good to go!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from os.path import join, dirname, basename, exists, isdir\n",
    "\n",
    "### Load environmental variables from the project root directory ###\n",
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# now you can get the variables using their names\n",
    "\n",
    "# Check whether a network drive has been specified\n",
    "DATABASE = os.environ.get(\"NETWORK_URL\")\n",
    "if DATABASE == 'None':\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "    #mount network drive here\n",
    "\n",
    "# set up directory paths\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJ = dirname(dotenv_path) # project root directory\n",
    "\n",
    "DATA = join(PROJ, 'data') #data directory\n",
    "RAW_EXTERNAL = join(DATA, 'raw_external') # external data raw directory\n",
    "RAW_INTERNAL = join(DATA, 'raw_internal') # internal data raw directory\n",
    "INTERMEDIATE = join(DATA, 'intermediate') # intermediate data directory\n",
    "FINAL = join(DATA, 'final') # final data directory\n",
    "\n",
    "RESULTS = join(PROJ, 'results') # output directory\n",
    "FIGURES = join(RESULTS, 'figures') # figure output directory\n",
    "PICTURES = join(RESULTS, 'pictures') # picture output directory\n",
    "\n",
    "\n",
    "folder_name = 'brenda_domains'\n",
    "if folder_name != '':\n",
    "    #make folders if they don't exist\n",
    "    if not exists(join(RAW_EXTERNAL, 'BRENDA_for_paper', folder_name)):\n",
    "        os.makedirs(join(RAW_EXTERNAL, 'BRENDA_for_paper', folder_name))\n",
    "    \n",
    "    if not exists(join(INTERMEDIATE, 'BRENDA_for_paper', folder_name)):\n",
    "        os.makedirs(join(INTERMEDIATE, 'BRENDA_for_paper', folder_name))\n",
    "\n",
    "print('Standard variables loaded, you are good to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from os.path import join, exists, getsize\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "I want to analyze, for each EC class, which uncharacterized sequences have the same domains as characterized ones. For this I download a Pfam flatfile and parse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pfam pre-calculated domain data (Version 33.1)\n",
    "This file is large, roughly 250 GB when unzipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ftp://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam33.1/Pfam-A.full.uniprot.gz'\n",
    "outfile = join(RAW_EXTERNAL, 'BRENDA_for_paper', 'Pfam-A.full.uniprot.gz')\n",
    "\n",
    "if not exists(outfile.replace('.gz', '')):\n",
    "    # download the hmm file\n",
    "    if not exists(outfile):\n",
    "        my_cmd = 'wget -O {} {}'.format(outfile, url)\n",
    "        os.system(my_cmd)\n",
    "\n",
    "    # unzip file\n",
    "    my_cmd = 'gunzip {}'.format(outfile)\n",
    "    os.system(my_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all uniprot identifiers from fasta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5072360 identifiers\n"
     ]
    }
   ],
   "source": [
    "def get_identifiers():\n",
    "    '''\n",
    "    Function for distributing the files on a number of workers.\n",
    "    '''\n",
    "    # define folder\n",
    "    inpath = join(INTERMEDIATE, 'BRENDA_for_paper', 'ec_identity_clustering')\n",
    "\n",
    "    # run assemble a list of the file to run\n",
    "    data = {}\n",
    "    for fi in sorted(os.listdir(inpath)):\n",
    "\n",
    "        if not fi.endswith('90_augmented.fasta'):\n",
    "            continue\n",
    "\n",
    "        # print(fi)\n",
    "\n",
    "        with open(join(inpath, fi), 'r') as f:\n",
    "            for line in f:\n",
    "                # only look at header lines\n",
    "                if line.startswith('>'):\n",
    "                    line_data = line.lstrip('>').rstrip().split(';')\n",
    "                    uid = line_data[0]\n",
    "\n",
    "                    data[uid] = []\n",
    "\n",
    "    print('{} identifiers'.format(len(data.keys())))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data = get_identifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through Pfam file to find the domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172358 sequences missing domain prediction\n"
     ]
    }
   ],
   "source": [
    "def get_doms(data, filepath):\n",
    "    '''\n",
    "    Try to find domains for the identifiers.\n",
    "    Uses pre-computed Pfam file.\n",
    "    '''\n",
    "    all_uid = set(data.keys())\n",
    "    start_found = False\n",
    "    counter = 0\n",
    "    with open(filepath, 'r', encoding=\"latin-1\") as f: #latin-1 to deal with a small number of byte codes in the file\n",
    "        for line in f:\n",
    "            # find beginning of a pfam domain record\n",
    "            if line.startswith('#=GF AC'):\n",
    "                if start_found is False: # if I find two starts in a row something went wrong\n",
    "                    pfam = line.split()[2].rstrip()\n",
    "                    start_found = True\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "            elif line.startswith('#'): # skip other annotation fields\n",
    "                continue\n",
    "\n",
    "            elif line.startswith('//'): # if I find the end before the start something went wrong\n",
    "                if start_found is True:\n",
    "                    start_found = False\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "            else:\n",
    "                uid = line.split('.')[0]\n",
    "                if uid in all_uid:\n",
    "                    data[uid].append(pfam)\n",
    "                    counter += 1\n",
    "\n",
    "    # sort the domains\n",
    "    for uid in data.keys():\n",
    "        data[uid] = sorted(data[uid])\n",
    "                    \n",
    "    print('found {} of {}'.format(counter, len(all_uid)))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def write_file(data, filepath):\n",
    "    '''\n",
    "    Write data to disk.\n",
    "    '''\n",
    "    outlines = []\n",
    "    for uid in sorted(data.keys()):\n",
    "        outlines.append('{}\\t{}'.format(uid, ','.join(data[uid])))\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('\\n'.join(outlines))\n",
    "\n",
    "\n",
    "        \n",
    "def load_file(filepath):\n",
    "    '''\n",
    "    Get data from disk.\n",
    "    '''\n",
    "    data =  {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            uid = parts[0]\n",
    "\n",
    "            if len(parts) == 1:\n",
    "                data[uid] = []\n",
    "            else:\n",
    "                domains = [s.split('.')[0] for s in sorted(parts[1].split(','))] # convert PF01676.19 to PF01676\n",
    "                data[uid] = domains\n",
    "    return data\n",
    "        \n",
    "        \n",
    "# parse the fasta files and get the uniprot identifiers\n",
    "outfile = join(INTERMEDIATE, 'BRENDA_for_paper', 'dom_temp_data.tsv')\n",
    "if not exists(outfile):\n",
    "    data = get_identifiers()\n",
    "\n",
    "    # parse the pfam file and get domains for the identifiers\n",
    "    data = get_doms(data, join(RAW_EXTERNAL, 'BRENDA_for_paper', 'Pfam-A.full.uniprot'))\n",
    "\n",
    "    # save data\n",
    "    write_file(data, outfile)\n",
    "    \n",
    "    \n",
    "# how many do not have a domain?\n",
    "data = load_file(outfile)\n",
    "print('{} sequences missing domain prediction'.format(list(data.values()).count([])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query UniProt for missing domains, but only for the characterized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/18032 [00:00<06:39, 45.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18032/18032 [02:11<00:00, 137.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_characterized():\n",
    "    # get a dictionary of characterized identifiers from BRENDA\n",
    "    characterized = {}\n",
    "    with open(join(INTERMEDIATE, 'BRENDA_for_paper', 'parsed_info', 'ec_data_uid_orgs.tsv'), 'r') as f:\n",
    "        f.readline()\n",
    "\n",
    "        for line in f:\n",
    "            ec, org, uid = line.rstrip().split('\\t')\n",
    "\n",
    "            if characterized.get(ec) is None:\n",
    "                characterized[ec] = set([])\n",
    "            characterized[ec].add(uid)\n",
    "            \n",
    "    # get all characterized identifiers from SwissProt\n",
    "    with open(join(RAW_EXTERNAL, 'SwissProt-2020_02-protein-evidence.tsv'), 'r') as f:\n",
    "        f.readline()\n",
    "        \n",
    "        for line in f:\n",
    "            uid, ec, org, orgid = line.strip().split('\\t')\n",
    "            \n",
    "            if characterized.get(ec) is None:\n",
    "                characterized[ec] = set([])\n",
    "            characterized[ec].add(uid)\n",
    "            \n",
    "    return characterized\n",
    "\n",
    "\n",
    "def dlfile(folder, filename, url):\n",
    "    '''\n",
    "    Download a web page if the corresponding file does not exist.\n",
    "    '''\n",
    "\n",
    "    # Open the url\n",
    "    try:\n",
    "        out_path = join(folder, filename)\n",
    "        if exists(out_path):\n",
    "#             print('already on disk ' + url)\n",
    "            return True\n",
    "            \n",
    "        elif not exists(out_path):\n",
    "            f = urlopen(url)\n",
    "            print(\"downloading \" + url)\n",
    "\n",
    "            # Open local file for writing\n",
    "            with open(out_path, \"wb\") as local_file:\n",
    "                local_file.write(f.read())\n",
    "            time.sleep(0.5)\n",
    "            return True\n",
    "\n",
    "    #handle errors\n",
    "    except HTTPError as e:\n",
    "        print(\"HTTP Error:\", e.code, url)\n",
    "        return False\n",
    "    \n",
    "    except URLError as e:\n",
    "        print(\"URL Error:\", e.reason, url)\n",
    "        return False\n",
    "        \n",
    "\n",
    "def get_pfam_from_file(filepath):\n",
    "    '''\n",
    "    Parse out pfam data from a file\n",
    "    '''\n",
    "    # open the uniprot and uniparc pages and append them\n",
    "    with open(filepath, 'r') as f:\n",
    "        document = f.read()\n",
    "\n",
    "        \n",
    "    # search through the combined document for identifiers\n",
    "    m = re.findall('(PF[0-9]{5}|CL[0-9]{4})', document)\n",
    "\n",
    "    # loop through the search result and keep unique identifiers\n",
    "    pfam_ids = set([])\n",
    "    for pid in m:\n",
    "        if pid == '':\n",
    "            continue\n",
    "                \n",
    "        pfam_ids.add(pid)\n",
    "        \n",
    "    return pfam_ids\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_pfam_for_uids(uids, filepath):\n",
    "    '''\n",
    "    Query the UniProt database to get protein domains for \n",
    "    a list of protein identifiers.\n",
    "    '''\n",
    "\n",
    "    data = {'uid':[], 'pfam':[]}\n",
    "\n",
    "    for uid in tqdm(uids):\n",
    "\n",
    "        # download uniprot file\n",
    "        uniprot_url = 'https://www.uniprot.org/uniprot/'\n",
    "        \n",
    "        result = dlfile(folder=filepath, filename='%s_uniprot.html' % uid, url=uniprot_url+uid)\n",
    "        if result is False:\n",
    "            continue\n",
    "\n",
    "        # query uniprot file\n",
    "        pfam_ids = get_pfam_from_file(join(filepath, '%s_uniprot.html' % uid))\n",
    "\n",
    "\n",
    "        # download uniparc overview file\n",
    "        uniparc_url = 'https://www.uniprot.org/uniparc/?query=%s' % uid\n",
    "        overview_filename = join(filepath, '%s_uniparc_overview.html' % uid)\n",
    "        result = dlfile(folder=filepath, filename='%s_uniparc_overview.html' % uid, url=uniparc_url)\n",
    "        if result is False:\n",
    "            continue\n",
    "\n",
    "        # find the reference for the alternate identifier and download that file\n",
    "        with open(overview_filename, 'r') as f:\n",
    "            document = f.read()\n",
    "\n",
    "            m = re.search('class=\"entryID\"><a href=\"/uniparc/([a-zA-Z0-9]+)\">', document)\n",
    "            if m is None:\n",
    "                continue\n",
    "                \n",
    "            new_target_url = 'https://www.uniprot.org/uniparc/%s' % m.group(1)\n",
    "            result = dlfile(folder=filepath, filename='%s_uniparc.html' % uid, url=new_target_url)\n",
    "            if result is False:\n",
    "                continue\n",
    "\n",
    "        # query uniparc file\n",
    "        pfam_ids2 = get_pfam_from_file(join(filepath, '%s_uniparc.html' % uid))\n",
    "\n",
    "\n",
    "        data['uid'].append(uid)\n",
    "        data['pfam'].append(','.join(sorted(list(pfam_ids.union(pfam_ids2)))))\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get characterized identfiers\n",
    "characterized = get_characterized()\n",
    "uids = set([])\n",
    "for ec in characterized.keys():\n",
    "    uids.update(characterized[ec])\n",
    "len(uids)\n",
    "\n",
    "# which characterized identifiers were not present in the Pfam file?\n",
    "missing = uids - set(data.keys())\n",
    "print(len(missing))\n",
    "\n",
    "# now download these\n",
    "filepath = join(RAW_EXTERNAL, 'pfam')\n",
    "if not exists(filepath):\n",
    "    os.makedirs(filepath)\n",
    "    \n",
    "df = get_pfam_for_uids(sorted(list(missing), reverse=False), filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the missing domains to the main data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173295 sequences missing domain prediction\n"
     ]
    }
   ],
   "source": [
    "data_missing = df.to_dict(orient='list')\n",
    "for key, value in zip(data_missing['uid'], data_missing['pfam']):\n",
    "    domains = sorted(value.split(','))\n",
    "    if domains == ['']:\n",
    "        domains = []\n",
    "    data[key] = domains\n",
    "\n",
    "# how many do not have a domain?\n",
    "print('{} sequences missing domain prediction'.format(list(data.values()).count([])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = join(FINAL, 'brenda_2019_2', 'all_brenda_domain_data.tsv')\n",
    "write_file(data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

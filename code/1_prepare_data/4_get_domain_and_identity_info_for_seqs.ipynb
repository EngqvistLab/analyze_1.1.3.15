{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for obtaining up-to-date protein domain information for the 1.1.3.15 sequences.<br/><br/>Copyright (C) 2020-2021  Martin Engqvist Lab<br/>This program is free software: you can redistribute it and/or modify<br/>it under the terms of the GNU General Public License as published by<br/>the Free Software Foundation, either version 3 of the License, or<br/>(at your option) any later version.<br/>This program is distributed in the hope that it will be useful,<br/>but WITHOUT ANY WARRANTY; without even the implied warranty of<br/>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the<br/>GNU General Public License for more details.<br/>You should have received a copy of the GNU General Public License<br/>along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard variables loaded, you are good to go!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from os.path import join, dirname, basename, exists, isdir\n",
    "\n",
    "### Load environmental variables from the project root directory ###\n",
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# now you can get the variables using their names\n",
    "\n",
    "# Check whether a network drive has been specified\n",
    "DATABASE = os.environ.get(\"NETWORK_URL\")\n",
    "if DATABASE == 'None':\n",
    "    pass\n",
    "else:\n",
    "    pass\n",
    "    #mount network drive here\n",
    "\n",
    "# set up directory paths\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJ = dirname(dotenv_path) # project root directory\n",
    "\n",
    "DATA = join(PROJ, 'data') #data directory\n",
    "RAW_EXTERNAL = join(DATA, 'raw_external') # external data raw directory\n",
    "RAW_INTERNAL = join(DATA, 'raw_internal') # internal data raw directory\n",
    "INTERMEDIATE = join(DATA, 'intermediate') # intermediate data directory\n",
    "FINAL = join(DATA, 'final') # final data directory\n",
    "\n",
    "RESULTS = join(PROJ, 'results') # output directory\n",
    "FIGURES = join(RESULTS, 'figures') # figure output directory\n",
    "PICTURES = join(RESULTS, 'pictures') # picture output directory\n",
    "\n",
    "\n",
    "folders = [join(RAW_EXTERNAL, 'brenda_2017_1'), \n",
    "           join(INTERMEDIATE, 'brenda_2017_1'), \n",
    "           join(FINAL, 'brenda_2017_1'), \n",
    "           join(RAW_EXTERNAL, 'brenda_2019_2'), \n",
    "           join(INTERMEDIATE, 'brenda_2019_2'), \n",
    "           join(FINAL, 'brenda_2019_2')]\n",
    "\n",
    "for f in folders:\n",
    "    if not exists(f):\n",
    "        os.makedirs(f)\n",
    "\n",
    "\n",
    "print('Standard variables loaded, you are good to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "import time\n",
    "from orgtools import topfunctions\n",
    "\n",
    "from Bio import SeqIO, AlignIO\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unirep.run_inference import BatchInference\n",
    "import tensorflow as tf\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The Pfam domains extracted in the previous notebook were only for the 2019_2 version of BRENDA, and only for sequences at the 90% identity level. To enable a detailed comparsison between the EC 1.1.3.15 enzyme family in the BRENDA versions 2017_1 and 2019_2 I go through all the identifiers and download up-to-date information directly from Pfam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract identifiers from the 2017.1 version of BRENDA\n",
    "Obtain information for these such as organism name, lineage, growth pH and growth temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outfiles exist, skipping.\n"
     ]
    }
   ],
   "source": [
    "def get_uid_from_fasta(filepath):\n",
    "    '''\n",
    "    Go through fasta file from BRENDA and extract the uid, organism and ec number.\n",
    "    '''\n",
    "    data = {}\n",
    "\n",
    "    # open file and go through it\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            # only look at header lines\n",
    "            if line.startswith('>'):\n",
    "                line_data = line.lstrip('>').rstrip().split(';')\n",
    "                uid = line_data[0]\n",
    "                ec = line_data[2]\n",
    "                org = ' '.join(line_data[3].split()[:2])\n",
    "\n",
    "                # add ec key if not present\n",
    "                if data.get(ec) is None:\n",
    "                    data[ec] = {}\n",
    "\n",
    "                if data[ec].get(org) is None:\n",
    "                    data[ec][org] = []\n",
    "\n",
    "                # add uid to data structure\n",
    "                data[ec][org].append(uid)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def write_outfile(data, filepath):\n",
    "    '''\n",
    "    Write the parsed data to an outfile.\n",
    "    '''\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write('uid\\tec\\n')\n",
    "\n",
    "        for ec in sorted(data.keys()):\n",
    "            for org in sorted(data[ec].keys()):\n",
    "                for entry in data[ec][org]:\n",
    "                    f.write('%s\\t%s\\n' % (entry, ec))\n",
    "\n",
    "\n",
    "\n",
    "def write_info_outfile(data, filepath):\n",
    "    '''\n",
    "    Write the parsed data to an outfile.\n",
    "    '''\n",
    "    # first collect all organism names\n",
    "    all_ids = []\n",
    "    for ec in sorted(data.keys()):\n",
    "        for org in sorted(data[ec].keys()):\n",
    "            all_ids.extend(data[ec][org])\n",
    "\n",
    "\n",
    "    # get taxid and lineage data\n",
    "    properties_object = topfunctions.Properties(all_ids)\n",
    "    properties_object.flatfile(filepath)\n",
    "\n",
    "\n",
    "\n",
    "infile = join(INTERMEDIATE, 'brenda_2017_1', '1_1_3__BRENDA_sequences_2017_1.fasta')\n",
    "outfile1 = join(FINAL, 'brenda_2017_1', 'ec_uid_org_from_fasta_2017_1.tsv')\n",
    "outfile2 = join(FINAL, 'brenda_2017_1', '1-1-3-n_identifier_info_2017_1.tsv')\n",
    "\n",
    "if (not exists(outfile1)) or (not exists(outfile2)):\n",
    "    data = get_uid_from_fasta(filepath=infile)\n",
    "    write_outfile(data, filepath=outfile1)\n",
    "    write_info_outfile(data, filepath=outfile2)\n",
    "\n",
    "else:\n",
    "    print('Outfiles exist, skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract identifiers from the 2019.2 version of BRENDA\n",
    "Obtain information for these such as organism name, lineage, growth pH and growth temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outfiles exist, skipping.\n"
     ]
    }
   ],
   "source": [
    "def make_fasta():\n",
    "    '''\n",
    "    Go through and collect sequnces from 1.1.3.n and\n",
    "    assemble into a new file.\n",
    "    '''\n",
    "    all_data = []\n",
    "    for fi in sorted(os.listdir(join(RAW_EXTERNAL, 'brenda_2019_2', 'sequence_data'))):\n",
    "        if fi.startswith('1.1.3.'):\n",
    "            with open(join(RAW_EXTERNAL, 'brenda_2019_2', 'sequence_data', fi), 'r') as f:\n",
    "                for line in f:\n",
    "                    all_data.append(line)\n",
    "\n",
    "    with open(join(INTERMEDIATE, 'brenda_2019_2', '1_1_3__BRENDA_sequences_2019_2.fasta'), 'w') as f:\n",
    "        f.write('\\n'.join(all_data))\n",
    "\n",
    "\n",
    "\n",
    "def filter_fasta(min_len = 200, max_len = 580):\n",
    "    '''Remove sequences that are too long or too short, also sequenes that have X in them'''\n",
    "    infile = join(INTERMEDIATE, 'brenda_2019_2', '1_1_3__BRENDA_sequences_2019_2.fasta')\n",
    "    outfile = join(INTERMEDIATE, 'brenda_2019_2', '1_1_3__BRENDA_sequences_filtered_2019_2.fasta')\n",
    "\n",
    "    retained = 0\n",
    "    removed_len = 0\n",
    "    removed_x = 0\n",
    "    removed_m = 0\n",
    "    with open(outfile, 'w') as f:\n",
    "        for record in SeqIO.parse(infile, 'fasta'):\n",
    "            header = record.description\n",
    "            seq = str(record.seq).lower()\n",
    "\n",
    "            if min_len <= len(seq) <= max_len and 'x' not in seq and seq[0] == 'm':\n",
    "                retained += 1\n",
    "                f.write('>%s\\n%s\\n' % (header, seq))\n",
    "            elif 'x' in seq:\n",
    "                removed_x += 1\n",
    "            elif seq[0] != 'm':\n",
    "                removed_m += 1\n",
    "            else:\n",
    "                removed_len += 1\n",
    "\n",
    "\n",
    "# create a fasta file with the 1.1.3.n sequences\n",
    "make_fasta()\n",
    "\n",
    "# filter out sequences that are too short or too long\n",
    "filter_fasta()\n",
    "\n",
    "\n",
    "infile = join(INTERMEDIATE, 'brenda_2019_2', '1_1_3__BRENDA_sequences_2019_2.fasta')\n",
    "outfile1 = join(FINAL, 'brenda_2019_2', 'ec_uid_org_from_fasta_2019_2.tsv')\n",
    "outfile2 = join(FINAL, 'brenda_2019_2', '1-1-3-n_identifier_info_2019_2.tsv')\n",
    "\n",
    "if (not exists(outfile1)) or (not exists(outfile2)):\n",
    "    data = get_uid_from_fasta(filepath=infile)\n",
    "    write_outfile(data, filepath=outfile1)\n",
    "    write_info_outfile(data, filepath=outfile2)\n",
    "\n",
    "else:\n",
    "    print('Outfiles exist, skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out only the 1.1.3.15 sequences from both database versions and get their UniRep representations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/projects/test/analyze_1.1.3.15/data/intermediate/brenda_2017_1/1_1_3__BRENDA_sequences_2017_1.fasta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-29937801f244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# filter fasta file to only retain 1.1.3.15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_fasta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mfilter_for_ec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fasta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_fasta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.1.3.15'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# compute the unreip representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-29937801f244>\u001b[0m in \u001b[0;36mfilter_for_ec\u001b[0;34m(infile, outfile, ec)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# the dictionary data structrue effectively removes (header) duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/annotation_error_test/lib/python3.7/site-packages/Bio/SeqIO/FastaIO.py\u001b[0m in \u001b[0;36mFastaIterator\u001b[0;34m(handle, alphabet, title2ids)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mas_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rU\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtitle2ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSimpleFastaParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/annotation_error_test/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/annotation_error_test/lib/python3.7/site-packages/Bio/File.py\u001b[0m in \u001b[0;36mas_handle\u001b[0;34m(handleish, mode, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandleish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/projects/test/analyze_1.1.3.15/data/intermediate/brenda_2017_1/1_1_3__BRENDA_sequences_2017_1.fasta'"
     ]
    }
   ],
   "source": [
    "def filter_for_ec(infile, outfile, ec):\n",
    "    '''\n",
    "    Take a fasta file as input and retain only sequences\n",
    "    belonging to a certain ec class. Also remove duplicates.\n",
    "    '''\n",
    "    # parse the fasta file and only keep sequenes from specified EC\n",
    "    # the dictionary data structrue effectively removes (header) duplicates\n",
    "    out_data = {}\n",
    "    for record in SeqIO.parse(infile, 'fasta'):\n",
    "        header = record.description\n",
    "        seq = record.seq\n",
    "        \n",
    "        if header.split(';')[2] == ec:\n",
    "            out_data[header] = seq\n",
    "       \n",
    "    # generate the output lines\n",
    "    outlines = []\n",
    "    for k, v in out_data.items():\n",
    "        outlines.append('>{}\\n{}'.format(k, v))\n",
    "    \n",
    "    # write to disk\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write('\\n'.join(outlines))\n",
    "\n",
    "\n",
    "def compute_unirep_representations(fasta_file, rep_file):\n",
    "    '''\n",
    "    Compute sequence representations from fasta file.\n",
    "    '''\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    inf_obj = BatchInference(batch_size=500)\n",
    "    df = inf_obj.run_inference(filepath=fasta_file)\n",
    "    df.to_csv(rep_file, sep='\\t')\n",
    "    \n",
    "    return df.values\n",
    "\n",
    "##########\n",
    "## 2017 ##\n",
    "##########\n",
    "raw_fasta = join(INTERMEDIATE, 'brenda_2017_1', '1_1_3__BRENDA_sequences_2017_1.fasta')\n",
    "filtered_fasta = join(INTERMEDIATE, 'brenda_2017_1', '1_1_3_15_BRENDA_sequences_2017_1.fasta')\n",
    "rep_file = join(INTERMEDIATE, 'brenda_2017_1', '1_1_3_15_BRENDA_sequences_2017_1_unirep.csv')\n",
    "\n",
    "# filter fasta file to only retain 1.1.3.15\n",
    "if not exists(filtered_fasta):\n",
    "    filter_for_ec(raw_fasta, filtered_fasta, ec='1.1.3.15')\n",
    "\n",
    "# compute the unreip representations\n",
    "if not exists(rep_file):\n",
    "    compute_unirep_representations(filtered_fasta, rep_file)\n",
    "\n",
    "\n",
    "##########\n",
    "## 2019 ##\n",
    "##########\n",
    "raw_fasta = join(INTERMEDIATE, 'brenda_2019_2', '1_1_3__BRENDA_sequences_2019_2.fasta')\n",
    "filtered_fasta = join(INTERMEDIATE, 'brenda_2019_2', '1_1_3_15_BRENDA_sequences_2019_2.fasta')\n",
    "rep_file = join(INTERMEDIATE, 'brenda_2019_2', '1_1_3_15_BRENDA_sequences_2019_2_unirep.csv')\n",
    "\n",
    "# filter fasta file to only retain 1.1.3.15\n",
    "if not exists(filtered_fasta):\n",
    "    filter_for_ec(raw_fasta, filtered_fasta, ec='1.1.3.15')\n",
    "\n",
    "# compute the unreip representations\n",
    "if not exists(rep_file):\n",
    "    compute_unirep_representations(filtered_fasta, rep_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute EC 1.1.3.15 identity matrix using pairwise alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of all selected\n",
    "def get_sequences():\n",
    "    '''\n",
    "    Get all sequencs as a dictionary\n",
    "    '''\n",
    "    seq_data = {}\n",
    "    infile = join(INTERMEDIATE, 'brenda_2017_1', '1_1_3_15_BRENDA_sequences_2017_1.fasta')\n",
    "    for record in SeqIO.parse(infile, 'fasta'):\n",
    "        header = record.description\n",
    "        seq = record.seq\n",
    "        seq_data[header] = str(seq)\n",
    "\n",
    "    return seq_data\n",
    "\n",
    "\n",
    "def pairwise_align(id1, id2, seq1, seq2):\n",
    "    '''\n",
    "    Perform pairwise alignment.\n",
    "    '''\n",
    "    records = '>%s\\n%s\\n>%s\\n%s' % (id1, seq1, id2, seq2) #prepare 'virtual' FASTA file\n",
    "\n",
    "    records_handle = io.StringIO(records) #turn string into a handle\n",
    "    tempdata = records_handle.getvalue()\n",
    "    muscle_cline = MuscleCommandline()\n",
    "    stdout, stderr = muscle_cline(stdin=tempdata)\n",
    "    aln = AlignIO.read(io.StringIO(stdout), \"fasta\")\n",
    "\n",
    "    output = []\n",
    "    for entry in aln:\n",
    "        output.append(entry.id.lstrip('>'))\n",
    "        output.append(entry.seq)\n",
    "    return output\n",
    "\n",
    "\n",
    "def pair_ident(Seq1, Seq2, single_gaps=True):\n",
    "    '''\n",
    "    Takes two aligned sequences and returns their percent identity.\n",
    "    Assumes that Seq1 and Seq2 are sequence strings\n",
    "    '''\n",
    "\n",
    "    l=0.0 # counts alignment length, excluding identical gaps, but including single gaps\n",
    "    n=0.0 # count number of single gaps\n",
    "    i=0.0 # counts identity hits\n",
    "\n",
    "\n",
    "    for j in range(len(Seq2)):\n",
    "        if Seq1[j] == '-' and Seq2[j] == '-': #DON'T count identical gaps towards alignment length\n",
    "            pass\n",
    "        else:\n",
    "            if Seq2[j] == Seq1[j]:\n",
    "                i += 1 #count matches\n",
    "            elif Seq2[j] == '-' or Seq1[j] == '-':\n",
    "                n += 1 #count number of single gaps\n",
    "            l += 1 #count total length with single gaps\n",
    "\n",
    "    if single_gaps is True: #include single gaps\n",
    "        percent = round(100*(i/l),1) #calculate identity\n",
    "\n",
    "    elif single_gaps is False: #exclude single gaps\n",
    "        if n >= l:\n",
    "            percent = 0.0\n",
    "        else:\n",
    "            percent = round(100*(i/(l-n)),1) #calculate identity\n",
    "\n",
    "    return percent\n",
    "\n",
    "\n",
    "def worker(package):\n",
    "    '''\n",
    "    '''\n",
    "    id1, seq1, id2, seq2 = package\n",
    "\n",
    "    # sometimes there is no closest match (i.e. no characterized sequence)\n",
    "    if id2 == 'None':\n",
    "        return (id1, id2, 0)\n",
    "    \n",
    "    # sequences that are too long make muscle fail, so ignore those\n",
    "    elif len(seq1) > 10000 or len(seq2) > 10000:\n",
    "        return (id1, id2, 0)\n",
    "\n",
    "\n",
    "    output = pairwise_align(id1='>%s'%id1, id2='>%s'%id2, seq1=seq1, seq2=seq2)\n",
    "\n",
    "    aln_seq1 = output[1]\n",
    "    aln_seq2 = output[3]\n",
    "\n",
    "    identity = pair_ident(Seq1=aln_seq1, Seq2=aln_seq2, single_gaps=True)\n",
    "    \n",
    "    return (id1, id2, identity)\n",
    "\n",
    "\n",
    "\n",
    "def get_packages():\n",
    "    '''\n",
    "    Generator for assembling sequence packages.\n",
    "    '''\n",
    "    # get all sequences\n",
    "    seq_data = get_sequences()\n",
    "    \n",
    "    # collect all the data, for later disribution to workeres\n",
    "    for i, id1 in enumerate(seq_data.keys()):\n",
    "        for j, id2 in enumerate(seq_data.keys()):\n",
    "            if i < j:\n",
    "                continue\n",
    "\n",
    "            seq1 = seq_data[id1]\n",
    "            seq2 = seq_data[id2]\n",
    "\n",
    "            # keep only the uniprot identifiers\n",
    "            uid1 = id1.split(';')[0]\n",
    "            uid2 = id2.split(';')[0]\n",
    "\n",
    "            # store the sequences and identifiers\n",
    "            yield ((uid1, seq1, uid2, seq2))\n",
    "\n",
    "            \n",
    "def compute_matrix():\n",
    "    '''\n",
    "    Do pairwise alignments and compute identities.\n",
    "    '''\n",
    "    # get all sequences\n",
    "    seq_data = get_sequences()\n",
    "\n",
    "    # generate all pairs and align them\n",
    "    total_alns = (len(seq_data.keys())**2-len(seq_data.keys())) / 2 + len(seq_data.keys())\n",
    "    print('Total {} alignments to do'.format(total_alns))\n",
    "\n",
    "    # carry out the alignments\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=num_cores) as pool:\n",
    "        results = list(tqdm(pool.imap(worker, get_packages()), total=total_alns))\n",
    "\n",
    "    # put the resulst into identiy matrix\n",
    "    identity_data = {}\n",
    "    for res in results:\n",
    "        uid1, uid2, identity = res\n",
    "\n",
    "        if identity_data.get(uid1) is None:\n",
    "            identity_data[uid1] = {}\n",
    "\n",
    "        if identity_data.get(uid2) is None:\n",
    "            identity_data[uid2] = {}\n",
    "\n",
    "        identity_data[uid1][uid2] = identity\n",
    "        identity_data[uid2][uid1] = identity\n",
    "            \n",
    "    return identity_data\n",
    "\n",
    "\n",
    "def save_identity_matrix_all(data, filepath):\n",
    "    '''\n",
    "    Save the identity data as a flatfile matrix.\n",
    "    '''\n",
    "    # Convert identity values to data frame and save to flatfile\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv(path_or_buf=filepath, sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "outfile = join(FINAL, 'brenda_2017_1', 'all_identity_matrix_ident.tsv')\n",
    "if not exists(outfile):\n",
    "    identity_data = compute_matrix()\n",
    "    save_identity_matrix_all(identity_data, filepath=outfile)\n",
    "else:\n",
    "    print('Outfile exists, skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PFAM domain info for sequences from BRENDA version 2017.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlfile(folder, filename, url):\n",
    "    '''\n",
    "    Download a web page if the corresponding file does not exist.\n",
    "    '''\n",
    "\n",
    "    # Open the url\n",
    "    try:\n",
    "        out_path = join(folder, filename)\n",
    "        if not exists(out_path):\n",
    "            f = urlopen(url)\n",
    "            print(\"downloading \" + url)\n",
    "\n",
    "            # Open local file for writing\n",
    "            with open(out_path, \"wb\") as local_file:\n",
    "                local_file.write(f.read())\n",
    "            time.sleep(1)\n",
    "\n",
    "    #handle errors\n",
    "    except HTTPError as e:\n",
    "        print(\"HTTP Error:\", e.code, url)\n",
    "    \n",
    "    except URLError as e:\n",
    "        print(\"URL Error:\", e.reason, url)\n",
    "        \n",
    "\n",
    "def get_pfam_from_file(filepath):\n",
    "    '''\n",
    "    Parse out pfam data from a file\n",
    "    '''\n",
    "    # open the uniprot and uniparc pages and append them\n",
    "    with open(filepath, 'r') as f:\n",
    "        document = f.read()\n",
    "\n",
    "        \n",
    "    # search through the combined document for identifiers\n",
    "    m = re.findall('(PF[0-9]{5}|CL[0-9]{4})', document)\n",
    "\n",
    "    # loop through the search result and keep unique identifiers\n",
    "    pfam_ids = set([])\n",
    "    for pid in m:\n",
    "        if pid == '':\n",
    "            continue\n",
    "                \n",
    "        pfam_ids.add(pid)\n",
    "        \n",
    "    return pfam_ids\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_pfam_for_uids(uids, filepath):\n",
    "    '''\n",
    "    Query the UniProt database to get protein domains for \n",
    "    a list of protein identifiers.\n",
    "    '''\n",
    "\n",
    "    data = {'uid':[], 'pfam':[]}\n",
    "\n",
    "    for uid in uids:\n",
    "\n",
    "        # download uniprot file\n",
    "        uniprot_url = 'https://www.uniprot.org/uniprot/'\n",
    "        dlfile(folder=filepath, filename='%s_uniprot.html' % uid, url=uniprot_url+uid)\n",
    "\n",
    "        # query uniprot file\n",
    "        pfam_ids = get_pfam_from_file(join(filepath, '%s_uniprot.html' % uid))\n",
    "\n",
    "\n",
    "        # download uniparc overview file\n",
    "        uniparc_url = 'https://www.uniprot.org/uniparc/?query=%s' % uid\n",
    "        overview_filename = join(filepath, '%s_uniparc_overview.html' % uid)\n",
    "        dlfile(folder=filepath, filename='%s_uniparc_overview.html' % uid, url=uniparc_url)\n",
    "\n",
    "        # find the reference for the alternate identifier and download that file\n",
    "        with open(overview_filename, 'r') as f:\n",
    "            document = f.read()\n",
    "\n",
    "            m = re.search('class=\"entryID\"><a href=\"/uniparc/([a-zA-Z0-9]+)\">', document)\n",
    "            new_target_url = 'https://www.uniprot.org/uniparc/%s' % m.group(1)\n",
    "            dlfile(folder=filepath, filename='%s_uniparc.html' % uid, url=new_target_url)\n",
    "\n",
    "        # query uniparc file\n",
    "        pfam_ids2 = get_pfam_from_file(join(filepath, '%s_uniparc.html' % uid))\n",
    "\n",
    "\n",
    "        data['uid'].append(uid)\n",
    "        data['pfam'].append(','.join(sorted(list(pfam_ids.union(pfam_ids2)))))\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = join(FINAL, 'brenda_2017_1', 'pfam_info_2017_1.tsv')\n",
    "if not exists(outfile):\n",
    "    # load a list of the identifiers I want\n",
    "    filepath = join(FINAL, 'brenda_2017_1', 'ec_uid_org_from_fasta_2017_1.tsv')\n",
    "    uid_ec = pd.read_csv(filepath, sep='\\t').drop_duplicates()\n",
    "\n",
    "    # only keep 1.1.3.15\n",
    "    data_subset = uid_ec[uid_ec['ec']=='1.1.3.15']\n",
    "    uids = data_subset.uid.values\n",
    "    \n",
    "    display(data_subset.head())\n",
    "    display(len(uids))\n",
    "\n",
    "    # download uniparc and uniprot for each\n",
    "    filepath = join(RAW_EXTERNAL, 'pfam')\n",
    "    if not exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    \n",
    "    data_frame = get_pfam_for_uids(uids, filepath)\n",
    "    data_frame.to_csv(outfile, sep='\\t', index=False)\n",
    "    \n",
    "else:\n",
    "    print('Outfile exists, skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download PFAM domain info for sequences from BRENDA version 2019.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = join(FINAL, 'brenda_2019_2', 'pfam_info_2019_2.tsv')\n",
    "if not exists(outfile):\n",
    "    # load a list of the identifiers I want\n",
    "    filepath = join(FINAL, 'brenda_2019_2', 'ec_uid_org_from_fasta_2019_2.tsv')\n",
    "    uid_ec = pd.read_csv(filepath, sep='\\t').drop_duplicates()\n",
    "\n",
    "    # only keep 1.1.3.15\n",
    "    data_subset = uid_ec[uid_ec['ec']=='1.1.3.15']\n",
    "    uids = data_subset.uid.values\n",
    "    \n",
    "    display(data_subset.head())\n",
    "    display(len(uids))\n",
    "\n",
    "    # download uniparc and uniprot for each\n",
    "    filepath = join(RAW_EXTERNAL, 'pfam')\n",
    "    if not exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    \n",
    "    data_frame = get_pfam_for_uids(uids, filepath)\n",
    "    data_frame.to_csv(outfile, sep='\\t', index=False)\n",
    "    \n",
    "else:\n",
    "    print('Outfile exists, skipping.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
